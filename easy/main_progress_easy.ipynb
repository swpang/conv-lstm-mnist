{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting\n",
    "Google drive mount (for Colab users) and package importing.\n",
    "You can optionally install and import torchensemble package for ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random  \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Sample Visualization\n",
    "You can see actual sample images and sorted class indices. Additional matplotlib package is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference: see actual samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alphabet = {\n",
    "        'A(a)' : '0', 'B(b)' : '1', 'C(c)' : '2', 'D(d)' : '3', 'E(e)' : '4', 'F(f)' : '5', \n",
    "        'G(g)' : '6', 'H(h)' : '7', 'I(i)' : '8', 'J(j)' : '9', 'K(k)' : '10','L(l)' : '11', \n",
    "        'M(m)' : '12', 'N(n)' : '13', 'O(o)' : '14', 'P(p)' : '15', 'Q(q)' : '16', 'R(r)' : '17', \n",
    "        'S(s)' : '18', 'T(t)' : '19', 'U(u)' : '20', 'V(v)' : '21', 'W(w)' : '22', 'X(x)' : '23', \n",
    "        'Y(y)' : '24', 'Z(z)' : '25'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress label:  4 6 8 10 12 14 \n",
      "( E(e) G(g) I(i) K(k) M(m) O(o) )\n"
     ]
    }
   ],
   "source": [
    "# Just for reference: see actual samples\n",
    "\n",
    "load_sample = np.load('./sample_data.npy', allow_pickle=True).item()\n",
    "sample_data, sample_label = load_sample['train_data'], load_sample['train_label']\n",
    "print(len(sample_data))\n",
    "\n",
    "plt.figure(figsize=(len(sample_data),len(sample_data)))\n",
    "for i in range(len(sample_data)):\n",
    "    plt.subplot(1, len(sample_data), i+1)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    plt.imshow(sample_data[i], cmap='gray')\n",
    "    \n",
    "plt.show()\n",
    "print(\"progress label: \", end=' ')\n",
    "label_str = '('\n",
    "\n",
    "for i in range(len(sample_label)):\n",
    "    print(int(sample_label[i]), end=' ')\n",
    "    label_str += \" \" + list(alphabet.keys())[int(sample_label[i])]\n",
    "label_str += \" )\"\n",
    "print()\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 0th GPU for training\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n"
     ]
    }
   ],
   "source": [
    "# fix random seed to increase reproducibility\n",
    "# NOTE: Do not modify here!\n",
    "\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "def seed_worker(worker_seed):\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "# you can modify this\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you can modify mean and std for normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.485), (0.230)),\n",
    "    #transforms.RandomRotation((-90,90), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    #transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "])\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 load from 0 to 5000\n",
      "1 load from 5000 to 10000\n",
      "2 load from 10000 to 15000\n",
      "3 load from 15000 to 20000\n",
      "4 load from 20000 to 25000\n",
      "5 load from 25000 to 30000\n",
      "6 load from 30000 to 35000\n",
      "7 load from 35000 to 40000\n",
      "8 load from 40000 to 45000\n",
      "9 load from 45000 to 50000\n",
      "0 load from 0 to 5000\n",
      "1 load from 5000 to 10000\n"
     ]
    }
   ],
   "source": [
    "# NOTE: modify path for your setting\n",
    "\n",
    "from data_utils import Mydataset, collate_fn\n",
    "\n",
    "train_path = './train'\n",
    "valid_path = './valid'\n",
    "\n",
    "train_ds = Mydataset(train_path, transform=transform, train=True)\n",
    "valid_ds = Mydataset(valid_path, transform=transform, train=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dl= DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, model_optim, loss_func, max_epoch, train_dl, valid_dl, \n",
    "          load_path=None, save_path='./model.pt'):\n",
    "    # Load your states\n",
    "    loaded_epoch = 0\n",
    "    loaded_best_acc = -1\n",
    "    if load_path is not None:\n",
    "        state = torch.load(load_path)\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        model_optim.load_state_dict(state[\"optimizer\"])\n",
    "        loaded_epoch = state[\"epoch\"]\n",
    "        loaded_best_acc = state[\"best_acc\"]\n",
    "        # ...\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_valid_accuracy = 0 if loaded_best_acc == -1 else loaded_best_acc\n",
    "    \n",
    "    for epoch in np.array(list(range(max_epoch - loaded_epoch))) + loaded_epoch:\n",
    "        n_samples = 0\n",
    "        n_correct = 0\n",
    "        model.train()\n",
    "        for step, sample in enumerate(train_dl):\n",
    "            img, label = sample\n",
    "                       \n",
    "            outputs = model((img, label))\n",
    "\n",
    "            # img(list) : Batch x [5, 1, 28, 28]\n",
    "            # x(tensor) : [batch * 5, 1, 28, 28]\n",
    "            # label(tensor): [batch, 6, 26]\n",
    "            # outputs(tensor): [batch, 26]\n",
    "            \n",
    "            model_optim.zero_grad()\n",
    "            \n",
    "            l = list(zip(*label))[-1]\n",
    "            l = torch.Tensor(l).cuda()\n",
    "            \n",
    "            loss = loss_func(outputs[:,-1,:].squeeze(), l.long())\n",
    "            \n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "\n",
    "            n_samples += len(label)\n",
    "            for j in range(len(label)):\n",
    "                    n_correct += (outputs[j].argmax(-1)[-1] == label[j][-1].cuda()).sum().item()        \n",
    "            \n",
    "            if (step + 1) % print_interval == 0:\n",
    "                print('epoch:', epoch + 1, 'step:', step + 1, 'loss:', loss.item(), 'accuracy:', 100 * (n_correct / n_samples))\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print('elapsed time : %d h %d m %d s' % (elapsed_time / 3600, (elapsed_time % 3600) / 60, (elapsed_time % 60)))    \n",
    "                \n",
    "        n_samples = 0\n",
    "        n_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, sample in enumerate(valid_dl):\n",
    "                img, label = sample\n",
    "                \n",
    "                outputs = model(img)\n",
    "                           \n",
    "                n_samples += len(label)\n",
    "                for j in range(len(label)):\n",
    "                    n_correct += (outputs[j].argmax(-1)[-1] == label[j][-1].cuda()).sum().item()        \n",
    "                    # print(f'{outputs[j,:].argmax(-1).item()}, {label[j,-1,:].argmax(-1)}')\n",
    "            valid_accuracy = 100 * (n_correct / n_samples)\n",
    "            if valid_accuracy > best_valid_accuracy:\n",
    "                state = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": model_optim.state_dict(),\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"best_acc\": best_valid_accuracy,\n",
    "                    # ...\n",
    "                }\n",
    "                torch.save(state, save_path)\n",
    "                best_valid_accuracy = valid_accuracy\n",
    "            print('Valid epoch: %d, Valid accuracy: %.2f, Best valid accuracy: %.2f' % (epoch + 1, valid_accuracy, best_valid_accuracy))\n",
    "\n",
    "def eval(valid_dl, load_path):\n",
    "    state = torch.load(load_path)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, sample in enumerate(valid_dl):\n",
    "            img, label = sample\n",
    "\n",
    "            outputs = model(img)\n",
    "                               \n",
    "            n_samples += len(label)\n",
    "            for j in range(len(label)):\n",
    "                n_correct += (outputs[j].argmax(-1)[-1] == label[j][-1].cuda()).sum().item()        \n",
    "                # print(f'{outputs[j,:].argmax(-1).item()}, {label[j,-1,:].argmax(-1)}')\n",
    "        \n",
    "    valid_accuracy = 100 * (n_correct / n_samples)\n",
    "    print('Valid accuracy: %.2f' % (valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add or modify your ConvLSTM's hyperparameter (keys and values)\n",
    "kwargs = {\n",
    "    'cnn_input_dim': 1,\n",
    "    'cnn_hidden_size': 16,\n",
    "    'rnn_input_dim': 64,\n",
    "    'rnn_hidden_size': 32,\n",
    "    'rnn_num_layers': 1,\n",
    "    'batch_size': batch_size\n",
    "}\n",
    "\n",
    "NUM_CLASSES = 26\n",
    "SEQUENCE_LENGTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (conv): CustomCNN(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=6272, out_features=64, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(\n",
      "    (lstm): LSTM(32, 32, batch_first=True)\n",
      "    (fc_in): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (fc_out): Linear(in_features=32, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# for reload .py file without restart\n",
    "import models_easy\n",
    "import importlib\n",
    "\n",
    "importlib.reload(models_easy)\n",
    "\n",
    "from models_easy import ConvLSTM\n",
    "\n",
    "model = ConvLSTM(sequence_length=SEQUENCE_LENGTH, num_classes=NUM_CLASSES, **kwargs).cuda()\n",
    "print(model)\n",
    "\n",
    "model_optim = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_interval = 50\n",
    "max_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 50 loss: 2.809053659439087 accuracy: 11.59375\n",
      "elapsed time : 0 h 0 m 2 s\n",
      "epoch: 1 step: 100 loss: 2.381230354309082 accuracy: 19.953125\n",
      "elapsed time : 0 h 0 m 5 s\n",
      "epoch: 1 step: 150 loss: 2.0876245498657227 accuracy: 25.330729166666664\n",
      "elapsed time : 0 h 0 m 8 s\n",
      "Valid epoch: 1, Valid accuracy: 47.50, Best valid accuracy: 47.50\n",
      "epoch: 2 step: 50 loss: 1.5658053159713745 accuracy: 53.31249999999999\n",
      "elapsed time : 0 h 0 m 15 s\n",
      "epoch: 2 step: 100 loss: 1.2258306741714478 accuracy: 58.5234375\n",
      "elapsed time : 0 h 0 m 18 s\n",
      "epoch: 2 step: 150 loss: 1.1282869577407837 accuracy: 62.99479166666667\n",
      "elapsed time : 0 h 0 m 21 s\n",
      "Valid epoch: 2, Valid accuracy: 83.64, Best valid accuracy: 83.64\n",
      "epoch: 3 step: 50 loss: 0.7198361158370972 accuracy: 85.984375\n",
      "elapsed time : 0 h 0 m 28 s\n",
      "epoch: 3 step: 100 loss: 0.5830205678939819 accuracy: 88.3671875\n",
      "elapsed time : 0 h 0 m 31 s\n",
      "epoch: 3 step: 150 loss: 0.47232484817504883 accuracy: 90.2734375\n",
      "elapsed time : 0 h 0 m 34 s\n",
      "Valid epoch: 3, Valid accuracy: 94.76, Best valid accuracy: 94.76\n",
      "epoch: 4 step: 50 loss: 0.3292398154735565 accuracy: 96.234375\n",
      "elapsed time : 0 h 0 m 41 s\n",
      "epoch: 4 step: 100 loss: 0.2916555404663086 accuracy: 96.546875\n",
      "elapsed time : 0 h 0 m 44 s\n",
      "epoch: 4 step: 150 loss: 0.2287241369485855 accuracy: 96.91666666666666\n",
      "elapsed time : 0 h 0 m 47 s\n",
      "Valid epoch: 4, Valid accuracy: 97.83, Best valid accuracy: 97.83\n",
      "epoch: 5 step: 50 loss: 0.1628386676311493 accuracy: 98.2578125\n",
      "elapsed time : 0 h 0 m 54 s\n",
      "epoch: 5 step: 100 loss: 0.18573273718357086 accuracy: 98.359375\n",
      "elapsed time : 0 h 0 m 57 s\n",
      "epoch: 5 step: 150 loss: 0.13736388087272644 accuracy: 98.4609375\n",
      "elapsed time : 0 h 1 m 0 s\n",
      "Valid epoch: 5, Valid accuracy: 98.58, Best valid accuracy: 98.58\n",
      "epoch: 6 step: 50 loss: 0.08798607438802719 accuracy: 99.109375\n",
      "elapsed time : 0 h 1 m 7 s\n",
      "epoch: 6 step: 100 loss: 0.10126397013664246 accuracy: 99.1484375\n",
      "elapsed time : 0 h 1 m 10 s\n",
      "epoch: 6 step: 150 loss: 0.09152231365442276 accuracy: 99.12760416666667\n",
      "elapsed time : 0 h 1 m 12 s\n",
      "Valid epoch: 6, Valid accuracy: 98.99, Best valid accuracy: 98.99\n",
      "epoch: 7 step: 50 loss: 0.0688796117901802 accuracy: 99.5\n",
      "elapsed time : 0 h 1 m 19 s\n",
      "epoch: 7 step: 100 loss: 0.06248430535197258 accuracy: 99.484375\n",
      "elapsed time : 0 h 1 m 22 s\n",
      "epoch: 7 step: 150 loss: 0.06427212059497833 accuracy: 99.44791666666667\n",
      "elapsed time : 0 h 1 m 25 s\n",
      "Valid epoch: 7, Valid accuracy: 99.14, Best valid accuracy: 99.14\n",
      "epoch: 8 step: 50 loss: 0.06480012834072113 accuracy: 99.609375\n",
      "elapsed time : 0 h 1 m 32 s\n",
      "epoch: 8 step: 100 loss: 0.06581375002861023 accuracy: 99.625\n",
      "elapsed time : 0 h 1 m 35 s\n",
      "epoch: 8 step: 150 loss: 0.05976914241909981 accuracy: 99.64322916666667\n",
      "elapsed time : 0 h 1 m 37 s\n",
      "Valid epoch: 8, Valid accuracy: 99.36, Best valid accuracy: 99.36\n",
      "epoch: 9 step: 50 loss: 0.039706990122795105 accuracy: 99.7578125\n",
      "elapsed time : 0 h 1 m 45 s\n",
      "epoch: 9 step: 100 loss: 0.03395668417215347 accuracy: 99.75390625\n",
      "elapsed time : 0 h 1 m 47 s\n",
      "epoch: 9 step: 150 loss: 0.03127485141158104 accuracy: 99.734375\n",
      "elapsed time : 0 h 1 m 50 s\n",
      "Valid epoch: 9, Valid accuracy: 99.25, Best valid accuracy: 99.36\n",
      "epoch: 10 step: 50 loss: 0.022047284990549088 accuracy: 99.8515625\n",
      "elapsed time : 0 h 1 m 57 s\n",
      "epoch: 10 step: 100 loss: 0.021505441516637802 accuracy: 99.86328125\n",
      "elapsed time : 0 h 2 m 0 s\n",
      "epoch: 10 step: 150 loss: 0.02130722813308239 accuracy: 99.84375\n",
      "elapsed time : 0 h 2 m 3 s\n",
      "Valid epoch: 10, Valid accuracy: 99.33, Best valid accuracy: 99.36\n"
     ]
    }
   ],
   "source": [
    "load_path = None\n",
    "train(model, model_optim, loss_func, max_epoch, train_dl, valid_dl, load_path=load_path, save_path='./model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy: 99.36\n"
     ]
    }
   ],
   "source": [
    "# load and evaluate model\n",
    "load_path = './model.pt'\n",
    "eval(valid_dl, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for grading by TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 load from 0 to 5000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/emnist_progress_easy_data/test/valid_data_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-95c4c1d722aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/emnist_progress_easy_data/test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMydataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/swpang/final-project/easy/data_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, transform, train)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'load from %d to %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mcur_load_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_data_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mcur_load_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_load_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_load_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_load_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/emnist_progress_easy_data/test/valid_data_0.npy'"
     ]
    }
   ],
   "source": [
    "# you do not need to modify here\n",
    "from data_utils import Mydataset, collate_fn\n",
    "\n",
    "test_path = './data/emnist_progress_easy_data/test'\n",
    "test_ds = Mydataset(test_path, transform=transform, train=False)\n",
    "test_dl= DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please change the model name to your submission model name\n",
    "load_path = './model.pt'\n",
    "eval(test_dl, load_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
